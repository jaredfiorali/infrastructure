name: popeye
namespace: popeye
image:
  repository: docker.io/derailed/popeye
  tag: v0.22.0@sha256:885992c4a6db77a9ffceddea840f6a4b29f60381d87d5ae74bd814af84f5b65d
deployment:
  enabled: false
resources:
  limits:
    cpu: '1'
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 500Mi
service:
  enabled: false
podDisruptionBudget:
  enabled: false
command:
  args: ["-A", "-o", "json", "--force-exit-zero", "-f", "/spinach.yaml", "--push-gtwy-url", "http://prometheus-prometheus-pushgateway.monitoring:9091"]
cronjob:
  enabled: true
  schedule: "*/10 * * * *"
volumeMounts:
  - name: popeye-configmap
    mountPath: /spinach.yaml
    subPath: spinach.yaml
    readOnly: true
  - name: tmp
    mountPath: /tmp
volumes:
  - name: popeye-configmap
    configMap:
      name: popeye-configmap
      defaultMode: 511
  - name: tmp
    emptyDir: {}
networkpolicy:
  egress:
    namespaces:
      - monitoring
containerSecurityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  privileged: false
podSecurityContext:
  runAsNonRoot: true
clusterRoleBinding:
  enabled: true
clusterRole:
  enabled: true
  rules:
    - apiGroups: [""]
      resources:
        - configmaps
        - endpoints
        - namespaces
        - nodes
        - persistentvolumes
        - persistentvolumeclaims
        - pods
        - secrets
        - serviceaccounts
        - services
        - events
      verbs: ["get", "list"]
    - apiGroups: ["apps"]
      resources:
        - daemonsets
        - deployments
        - statefulsets
        - replicasets
      verbs: ["get", "list"]
    - apiGroups: ["networking.k8s.io"]
      resources:
        - ingresses
        - networkpolicies
      verbs: ["get", "list"]
    - apiGroups: ["batch"]
      resources:
        - cronjobs
        - jobs
      verbs: ["get", "list"]
    - apiGroups: ["gateway.networking.k8s.io"]
      resources:
        - gatewayclasses
        - gateways
        - httproutes
      verbs: ["get", "list"]
    - apiGroups: ["autoscaling"]
      resources:
        - horizontalpodautoscalers
      verbs: ["get", "list"]
    - apiGroups: ["policy"]
      resources:
        - poddisruptionbudgets
        - podsecuritypolicies
      verbs: ["get", "list"]
    - apiGroups: ["rbac.authorization.k8s.io"]
      resources:
        - clusterroles
        - clusterrolebindings
        - roles
        - rolebindings
      verbs: ["get", "list"]
    - apiGroups: ["metrics.k8s.io"]
      resources:
        - pods
        - nodes
      verbs: ["get", "list"]
prometheusRules:
  enabled: true
  rules:
    - alert: PopeyeLowScore
      annotations:
        description: If the popeye linter has a low score, it indicates that the cluster is not healthy.
        summary: Popeye linter score is low.
      expr: popeye_report_score < 100
      for: 5m
      keep_firing_for: 10m
      labels:
        issue: Popeye score is less than 100.
        severity: critical
imageUpdater:
  enabled: true
  rootImageTag: "v0.22.0"
configMap:
  enabled: true
  value: |-
    spinach.yaml: |
      popeye:
        # Popeye codes can be found here: https://popeyecli.io/docs/codes.html
        #
        # Checks resources against reported metrics usage.
        # If over/under these thresholds a linter warning will be issued.
        # Your cluster must run a metrics-server for these to take place!
        allocations:
          cpu:
            underPercUtilization: 200 # Checks if cpu is under allocated by more than 200% at current load.
            overPercUtilization:  50  # Checks if cpu is over allocated by more than 50% at current load.
          memory:
            underPercUtilization: 200 # Checks if mem is under allocated by more than 200% at current load.
            overPercUtilization:  50  # Checks if mem is over allocated by more than 50% usage at current load.
        # Excludes excludes certain resources from Popeye scans
        excludes:
          global:
            fqns: [rx:^kube-, rx:^loki, rx:^longhorn-system, rx:^argocd, rx:^metallb-system] # => excludes all third party resources
            labels:
              app.kubernetes.io/part-of: [kube-state-metrics, kube-prometheus-stack, prometheus-node-exporter] # => excludes all third party resources
              app.kubernetes.io/managed-by: [prometheus-operator] # => excludes all third party resources
              managed-by: [prometheus-operator] # => excludes all third party resources
              app.kubernetes.io/name: [prometheus-pushgateway] # => excludes all third party resources
              skip-popeye: ["true"] # => excludes all resources with labels skip-popeye=true
            # [Sample]
            annotations:
              fred: [blee, duh] # => exclude any resources with annotations matching either fred=blee or fred=duh
            # => exclude specified codes (https://popeyecli.io/docs/codes.html)
            codes: ["1109", "406"] # => Only one Pod associated with this endpoint, K8s version OK
          # Configure individual resource linters
          linters:
            clusterrolebindings:
              instances:
                - fqns: [system:kube-dns, system:controller:route-controller, system:controller:selinux-warning-controller, system:controller:service-controller]
                  codes: [1300] # => References a ServiceAccount which does not exist
            # Configure the namespaces linter for v1/namespaces
            namespaces:
              # Exclude these codes for all namespace resources straight up or via regex.
              codes: ["1109"] # => Only one Pod associated with this endpoint
              #  Excludes specific namespaces from the scan
              instances:
                - fqns: [kube-public, kube-system] # => skip ns kube-public and kube-system
            cronjobs:
              instances:
                - labels:
                    app: [popeye, git-sync, kometa]
                  codes: [1501] # => Cronjob not ready
            secrets:
              instances:
                - labels:
                    app: [git-sync, kometa]
                  codes: [400] # => Secret not assigned
            services:
              instances:
                - labels:
                    app: [cloudflared, plex]
                  codes: [1103, 1107] # Load balancer services
            serviceaccounts:
              instances:
                - labels:
                    app: [git-sync, kometa]
                  codes: [400] # Unable to locate resource reference
            configmaps:
              instances:
                - labels:
                    app: [git-sync, kometa]
                  codes: [400] # Unable to locate resource reference
            networkpolicies:
              instances:
                # - fqns: [rx:^downloader]
                #   codes: [1207, 1206]
                - labels:
                    app: [git-sync, kometa, popeye]
                  codes: [1200] # => No pods match network policy
                - labels:
                    app: [homepage, home-assistant, influxdb, linkding, scrypted, sillytavern, unifi-poller]
                  codes: [1206] # => Can't match 192.168.0.0/16 (ingress)
                - labels:
                    app: [git-sync, kometa, linkding, cloudflared, grafana, plex, prowlarr, radarr, ssh, sonarr, transmission, tautulli]
                  codes: [1207] # => Can't match 192.168.0.0/16 (egress)
            # Configure the pods linter for v1/pods.
            pods:
              instances:
                - labels:
                    app: [dawarich-app, emulatorjs, plex, transmission] # Exclude codes for any pods with specified labels
                  codes: [302] # => Pod could be running as root user. Check SecurityContext/Image
                - labels:
                    app: [dawarich-app, emulatorjs, plex, transmission, gluetun, ping] # Exclude codes for any pods with specified labels
                  codes: [306] # => Container could be running as root user. Check SecurityContext/Image
                - labels:
                    app: [popeye, git-sync, kometa] # Exclude codes for any pods with specified labels
                  codes: [203, 206, 207] # => Pod is waiting, Pod missing Pdb, Pod is unhappy
                - labels:
                    app: [radarr, sonarr, transmission] # Exclude codes for any pods with specified labels
                  codes: [110] # => Memory Current/Request threshold
        resources:
          # Configure node resources.
          node:
            # Limits set a cpu/mem threshold in % ie if cpu|mem > limit a lint warning is triggered.
            limits:
              # CPU checks if current CPU utilization on a node is greater than 90%.
              cpu:    90
              # Memory checks if current Memory utilization on a node is greater than 80%.
              memory: 80
          # Configure pod resources
          pod:
            # Restarts check the restarts count and triggers a lint warning if above threshold.
            restarts: 3
            # Check container resource utilization in percent.
            # Issues a lint warning if about these threshold.
            limits:
              cpu:    80
              memory: 75
        # [New!] overrides code severity
        overrides:
          # Code specifies a custom severity level ie critical=3, warn=2, info=1
          # - code: 1206
          #   severity: 1
        # Configure a list of allowed registries to pull images from.
        # Any resources not using the following registries will be flagged!
        registries:
          - quay.io
          - docker.io
          - ghcr.io
