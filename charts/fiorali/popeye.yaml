name: popeye
namespace: popeye
image:
  repository: docker.io/derailed/popeye
  tag: v0.22.0@sha256:885992c4a6db77a9ffceddea840f6a4b29f60381d87d5ae74bd814af84f5b65d
deployment:
  enabled: false
resources:
  limits:
    cpu: '1'
    memory: 1Gi
service:
  enabled: false
podDisruptionBudget:
  enabled: false
command:
  enabled: true
  exec: ["/bin/sh", "-c", "popeye", "-A", "-o", "json", "--force-exit-zero", "-f", "/spinach.yaml", "--push-gtwy-url", "http://prometheus-prometheus-pushgateway.monitoring:9091"]
cronjob:
  enabled: true
  schedule: "*/10 * * * *"
volumeMounts:
- name: popeye-configmap
  mountPath: /spinach.yaml
  subPath: spinach.yaml
  readOnly: true
- name: tmp
  mountPath: /tmp
volumes:
- name: popeye-configmap
  configMap:
    name: popeye-configmap
    defaultMode: 511
- name: tmp
  emptyDir: {}
networkpolicy:
  egress:
    namespaces:
    - monitoring
containerSecurityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  privileged: false
podSecurityContext:
  fsGroup: 2000
  runAsNonRoot: true
clusterRoleBinding:
  enabled: true
clusterRole:
  enabled: true
  rules:
  - apiGroups: [""]
    resources:
    - configmaps
    - endpoints
    - namespaces
    - nodes
    - persistentvolumes
    - persistentvolumeclaims
    - pods
    - secrets
    - serviceaccounts
    - services
    - events
    verbs: ["get", "list"]
  - apiGroups: ["apps"]
    resources:
    - daemonsets
    - deployments
    - statefulsets
    - replicasets
    verbs: ["get", "list"]
  - apiGroups: ["networking.k8s.io"]
    resources:
    - ingresses
    - networkpolicies
    verbs: ["get", "list"]
  - apiGroups: ["batch"]
    resources:
    - cronjobs
    - jobs
    verbs: ["get", "list"]
  - apiGroups: ["gateway.networking.k8s.io"]
    resources:
    - gatewayclasses
    - gateways
    - httproutes
    verbs: ["get", "list"]
  - apiGroups: ["autoscaling"]
    resources:
    - horizontalpodautoscalers
    verbs: ["get", "list"]
  - apiGroups: ["policy"]
    resources:
    - poddisruptionbudgets
    - podsecuritypolicies
    verbs: ["get", "list"]
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources:
    - clusterroles
    - clusterrolebindings
    - roles
    - rolebindings
    verbs: ["get", "list"]
  - apiGroups: ["metrics.k8s.io"]
    resources:
    - pods
    - nodes
    verbs: ["get", "list"]
prometheusRules:
  enabled: true
  rules:
  - alert: PopeyeLowScore
    annotations:
      description: If the popeye linter has a low score, it indicates that the cluster is not healthy.
      summary: Popeye linter score is low.
    expr: popeye_report_score < 100
    for: 5m
    keep_firing_for: 10m
    labels:
      issue: Popeye score is less than 100.
      severity: critical
  - alert: PopeyeErrorsDetected
    annotations:
      description: If the popeye linter has detected errors, it indicates that the cluster is not healthy.
      summary: Popeye has detected errors.
    expr: popeye_severity_total{severity="error"} > 0
    for: 5m
    keep_firing_for: 10m
    labels:
      issue: Popeye has detected errors in the {{$labels.namespace}} namespace.
      severity: critical
configMap:
  enabled: true
  data:
  name: spinach.yaml
  value: |-
    popeye:
      # Checks resources against reported metrics usage.
      # If over/under these thresholds a linter warning will be issued.
      # Your cluster must run a metrics-server for these to take place!
      allocations:
        cpu:
          underPercUtilization: 200 # Checks if cpu is under allocated by more than 200% at current load.
          overPercUtilization:  50  # Checks if cpu is over allocated by more than 50% at current load.
        memory:
          underPercUtilization: 200 # Checks if mem is under allocated by more than 200% at current load.
          overPercUtilization:  50  # Checks if mem is over allocated by more than 50% usage at current load.

      # Excludes excludes certain resources from Popeye scans
      excludes:
        global:
          fqns: [rx:^kube-, rx:^loki, rx:^longhorn-system, rx:^argocd] # => excludes all third party resources
          labels:
            app.kubernetes.io/part-of: [kube-state-metrics, kube-prometheus-stack, prometheus-node-exporter] # => excludes all third party resources
            app.kubernetes.io/managed-by: [prometheus-operator] # => excludes all third party resources
            managed-by: [prometheus-operator] # => excludes all third party resources
            app.kubernetes.io/name: [prometheus-pushgateway] # => excludes all third party resources
            skip-popeye: ["true"] # => excludes all resources with labels skip-popeye=true
          # [Sample]
          annotations:
            fred: [blee, duh] # => exclude any resources with annotations matching either fred=blee or fred=duh
          # => exclude specified codes (https://popeyecli.io/docs/codes.html)
          codes: ["1109", "406"] # => Only one Pod associated with this endpoint, K8s version OK

        # Configure individual resource linters
        linters:
          # Configure the namespaces linter for v1/namespaces
          namespaces:
            # Exclude these codes for all namespace resources straight up or via regex.
            codes: ["1109"] # => Only one Pod associated with this endpoint
            #  Excludes specific namespaces from the scan
            instances:
              - fqns: [kube-public, kube-system] # => skip ns kube-public and kube-system

          cronjobs:
            instances:
              - labels:
                  app: [popeye, git-sync]
                codes: [1501] # => Cronjob not ready

          secrets:
            instances:
              - labels:
                  app: [git-sync]
                codes: [400] # => Secret not assigned

          services:
            instances:
              - labels:
                  app: [timemachine, ssh, plex]
                codes: [1103] # Load balancer services

          serviceaccounts:
            instances:
              - labels:
                  app: [git-sync]
                codes: [400] # Unable to locate resource reference

          configmaps:
            instances:
              - labels:
                  app: [git-sync]
                codes: [400] # Unable to locate resource reference

          networkpolicies:
            instances:
              # - fqns: [rx:^downloader]
              #   codes: [1207, 1206]
              - labels:
                  app: [git-sync, popeye]
                codes: [1200] # => No pods match network policy
              - labels:
                  app: [homepage, home-assistant, influxdb, linkding, scrypted, sillytavern, unifi-poller]
                codes: [1206] # => Can't match 192.168.0.0/16 (ingress)
              - labels:
                  app: [git-sync, linkding, plex, prowlarr, radarr, ssh, sonarr, transmission, tautulli]
                codes: [1207] # => Can't match 192.168.0.0/16 (egress)

          # Configure the pods linter for v1/pods.
          pods:
            instances:
              # - fqns: [rx:^dawarich, rx:^downloader, rx:^emulatorjs, rx:^home-assistant, rx:^timemachine]
              #   codes: [306] # => skip code 207 for namespace popeye
              - labels:
                  app: [dawarich-app, dawarich-sidekiq, emulatorjs, home-assistant, plex, prowlarr, radarr, ssh, sonarr, timemachine, transmission] # Exclude codes for any pods with specified labels
                codes: [302] # => Pod could be running as root user. Check SecurityContext/Image
              - labels:
                  app: [dawarich-app, dawarich-sidekiq, emulatorjs, gluetun, home-assistant, ping, plex, prowlarr, radarr, ssh, sonarr, timemachine, transmission] # Exclude codes for any pods with specified labels
                codes: [306] # => Container could be running as root user. Check SecurityContext/Image
              - labels:
                  app: [popeye, git-sync] # Exclude codes for any pods with specified labels
                codes: [203, 206, 207] # => Pod is waiting, Pod missing Pdb, Pod is unhappy

      resources:
        # Configure node resources.
        node:
          # Limits set a cpu/mem threshold in % ie if cpu|mem > limit a lint warning is triggered.
          limits:
            # CPU checks if current CPU utilization on a node is greater than 90%.
            cpu:    90
            # Memory checks if current Memory utilization on a node is greater than 80%.
            memory: 80

        # Configure pod resources
        pod:
          # Restarts check the restarts count and triggers a lint warning if above threshold.
          restarts: 3
          # Check container resource utilization in percent.
          # Issues a lint warning if about these threshold.
          limits:
            cpu:    80
            memory: 75


      # [New!] overrides code severity
      overrides:
        # Code specifies a custom severity level ie critical=3, warn=2, info=1
        # - code: 1206
        #   severity: 1

      # Configure a list of allowed registries to pull images from.
      # Any resources not using the following registries will be flagged!
      registries:
        - quay.io
        - docker.io
        - ghcr.io